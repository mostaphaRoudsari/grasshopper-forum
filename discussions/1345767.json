{"body": "\nDear Mostapha and dear everyone!\n\nWe found some problems with LB and HB which look like bugs.\n\nSome are easy to fix and show up very often.\n\nTo fix them we have to disable and reactivate or even replaced various items and let the HB and LB fly again and again.\n\nWith growing complexity of the Grasshopper canvas some unpredictable behaviors of LBHB are increasing.\n\nThe result of an annual cumulative irradiation study is not always consistent when performed with LBHB but is credible when performed with Diva on the very same Grasshopper file.\n\nTo trigger the bug just change the value of one of the inputs without set the HB toggle to \"False\" but leaving it on \"True\"\n\nWe noticed that when the bug happens the simulation in radiance is really quick.If the toggle on the HB simulation is switched\u00a0 \"False\" and then switched \"True\" again the simulation produces meaningful results.\n\nWe can solve this problem by switching the toggle, but the number of unpredictable behaviors makes us uneasy.\n\nIs it for other users as well or is a problem specific of some computers?\n\nThere may be problems that we did not detect?\n\nThank you very much!\n\n\n\n\n\n\n", "attachments": [{"link": "http://www.grasshopper3d.com/group/ladybug/forum/attachment/download?id=2985220%3AUploadedFile%3A1345766", "name": "HB simulation.gh"}], "created_by_name": "Jennifer Addams", "created_at": "August 19, 2015 at 9:32am", "created_by": "JenniferAddams", "topic": "unpredictable behaviour of LBHB components", "replies": [{"body": "\nHi Jennifer,\n\n\"To fix them we have to disable and reactivate or even replaced various items and let the HB and LB fly again and again.\"\n\n\u00a0\n\nThis happens because of the order of inserting components in Grasshopper. The easy fix is to select Ladybug component, and Honeybee component separately and press ctrl+B which will send the components back in the order and they will be executed first when you open the file.\n\n\n\nThe result of an annual cumulative irradiation study is not always consistent when performed with LBHB but is credible when performed with Diva on the very same Grasshopper file.\n\nTo trigger the bug just change the value of one of the inputs without set the HB toggle to \"False\" but leaving it on \"True\"\n\nWe noticed that when the bug happens the simulation in radiance is really quick.If the toggle on the HB simulation is switched\u00a0 \"False\" and then switched \"True\" again the simulation produces meaningful results.\n\n\n\nIt sounds like when you trigger the analysis it doesn't run the first time. It's not normal and I haven't experienced this before. Other users can comment on this. I you upload the file more people can test your file.\n\n\n\nMostapha\n\n\n\n\n", "attachments": [], "created_by_name": "Mostapha Sadeghipour Roudsari", "created_at": "August 19, 2015 at 12:11pm", "created_by": "MostaphaSadeghipour", "replies": [{"body": "\nDear Mostapha,\n\nthank you for the reply :)\n\nwhich file you are referring to?\n\ni attached \"HB simulation.gh\" in that the problem occurs\n\nThank You Again\n\nJennifer\n", "attachments": [], "created_by_name": "Jennifer Addams", "created_at": "August 20, 2015 at 1:15am", "created_by": "JenniferAddams", "replies": []}, {"body": "\nDear Mostapha,\n\nwe found the origin of the problem.\n\nis connected with the \"rad parameters\" component.\n\nif you plug an empty \"rad parameters component\" in the \"grd component \" in the file attached to the original message everything works\n\n[<img class=\"align-full\" src=\"http://api.ning.com:80/files/bGawF3RRkafjqPxdK2akptD8gzJ9-DinO8upt0RsrqdHqUFS2ezQpLIfTyeWnp*UeBQCqpKbUf63S3IJxB8RuYaSg9OFYcjT/works.JPG?width=721\" width=\"721\"/>](http://api.ning.com:80/files/bGawF3RRkafjqPxdK2akptD8gzJ9-DinO8upt0RsrqdHqUFS2ezQpLIfTyeWnp*UeBQCqpKbUf63S3IJxB8RuYaSg9OFYcjT/works.JPG)instead if you leave the slot empty it works only by switching false and true again in the \"RunDaylightAnalysis\" component\n\n[<img class=\"align-full\" src=\"http://api.ning.com:80/files/bGawF3RRkaefYirkaFEHdDN1WQksC4mUbC0lYLI6x*3pCITr8aKbgorKK4u7zknBbhHzAtIQ*BijKigjdFENQugAUfgwncQG/noworks.JPG?width=721\" width=\"721\"/>](http://api.ning.com:80/files/bGawF3RRkaefYirkaFEHdDN1WQksC4mUbC0lYLI6x*3pCITr8aKbgorKK4u7zknBbhHzAtIQ*BijKigjdFENQugAUfgwncQG/noworks.JPG)Hope it helps\n\nJennifer\n\n\n", "attachments": [], "created_by_name": "Jennifer Addams", "created_at": "August 20, 2015 at 9:10am", "created_by": "JenniferAddams", "replies": []}, {"body": "\nJennifer,\n\nAfter playing around with your file a bit, I believe that I understand the specific issue that you are concerned about and I have a best guess for what is going on under the hood (Mostapha might be able to provide more insight). \u00a0Whenever you run a new simulation in Radiance, it is not always necessary to re-write all of the initial simulation files from scratch. \u00a0These initial simulation files include both a .rad geometry file as well as a separate .pts file that contains the test point locations. \u00a0If all that you are changing in a given parametric run is the locations of the test points (like your case), it is not necessary to re-write (or reinterpret) the entire .rad geometry file. \u00a0My guess is that there is some type of check for this built into either code Mostapha wrote or radiance functions that Mostapha is calling. As such, it seems that the rad geometry file is not being re-written (or re-interpreted by radiance) completely when all that you change is the test points and this actually seems to be saving you an extra 10 seconds each time that you run the component without changing the materials or the building geometry. \u00a0Other times (like when you plug in custom radParameters), it seems that it re-writes (or re-interprets) the .rad geometry file from scratch since this file is probably affected by customized rad parameters.\n\n\n\nSo far, if this explanation is holding, it seems like there would be no concern on your end but I also recognize that the difference between these long and short simulations is giving you radiation results that are ever so slightly different from each other (by my estimates, they differ by about 0.2%). \u00a0Compared to the other types of assumptions that the radiance model is making, though, these are mere rounding errors that probably originate from the number of decimal places in the vertices of the rad geometry file. \u00a0Rather than worrying about whether your simulations are giving you the right rounding errors to give you matching results, I would encourage you to instead contemplate how much your radiance results are matching reality given all of the assumptions that you are making about the climate (with the epw file for a \"typical\" year) and with the number of light bounces in the radiance simulation. \u00a0To give you an example, I ran your model with a higher quality of simulation type (3 ambient bounces) and this gives you results that differ by 1.1% from the original simulation that you were running with only 2 ambient bounces (this is practically an order of magnitude larger than 0.2%).\n\n\n\nTo address your unease I will say that, for a long time, I also felt uneasy any time that I encountered something that seemed unpredictable in software that I was using. \u00a0Once I started coding my own stuff, though, I realized quickly that unpredictable behavior is an unavoidable aspect of all software. \u00a0There is always a tradeoff between accurate results and the time it takes to get them, which produces a multitude of possible ways to arrive at a solution. \u00a0Add into this complex situation the fact that you might have an almost infinite number of possible inputs to a given set of code.\n\n\n\nBecause of the unpredictable\u00a0multitude of cases, there is no application that is completely free from limitations and assumptions. \u00a0In this light, what ends up being more important than the actual calculation method used is the social infrastructure that is in place to help understand what is being run under the hood, hence why both Radiance and Honeybee are open source and why we try to build a robust community of support through forums like this one!\n\n-Chris\n", "attachments": [], "created_by_name": "Chris Mackey", "created_at": "August 22, 2015 at 3:46pm", "created_by": "ChrisMackey", "replies": []}, {"body": "\nHi,\n\nJust to strenghten Chris's comments and without checking the files. Radiance simulations are stochastic , so there is a factor of randomness in the simulations. That's why the same simulation can give slightly different results. There are plenty of comments on this. Not sure that in this forum, but for sure in others.\n\nSo if this is the case, i'll be fine with this 0.2% difference for the same parameters.\n\n-A.\n", "attachments": [], "created_by_name": "Abraham Yezioro", "created_at": "August 22, 2015 at 9:05pm", "created_by": "AbrahamYezioro", "replies": []}, {"body": "\nI tested the file and the results looks fine to me. As Abraham mentioned Radiance uses stochastic sampling and up to 2-5% difference in runs is pretty normal. Here is a post on Radiance group for your reference: \u00a0[http://www.radiance-online.org/pipermail/radiance-general/2012-Octo...](http://www.radiance-online.org/pipermail/radiance-general/2012-October/008921.html)\n", "attachments": [], "created_by_name": "Mostapha Sadeghipour Roudsari", "created_at": "August 23, 2015 at 1:23pm", "created_by": "MostaphaSadeghipour", "replies": []}]}]}