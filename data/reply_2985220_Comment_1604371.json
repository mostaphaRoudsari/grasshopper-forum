{"body": "\nTimothy,\n\nI knew it was only a matter of time before someone brought this up. \u00a0The heart of the issue is that, each time a Honeybee component runs that alters HBZones, there is an entirely new copy of the zone object being made. \u00a0So, if you have a large model, with many components that alter the zones, and try to run several iterations of it, you can find yourself writing a lot of copies of zones to your memory quickly, which will max out your memory in the way you describe.\n\nBecause of this, I have actually gotten a lot of mileage out of upgrading my computer from 16 GB to 32 GB. \u00a0Still, as Mostapha suggests, there are a number of intelligent ways of laying out the GH script to minimize the copies of zones that get written to memory and will buy you more iterations before max-out. \u00a0If you upload the GH file, we can understand the memory pressure points.\n\n-Chris\n", "attachments": [], "created_by_name": "Chris Mackey", "created_at": "September 22, 2016 at 10:48pm", "created_by": "chris", "parent_id": "reply_2985220_Comment_1604261", "id": "reply_2985220_Comment_1604371"}