{"body": "\nJennifer,\n\nAfter playing around with your file a bit, I believe that I understand the specific issue that you are concerned about and I have a best guess for what is going on under the hood (Mostapha might be able to provide more insight). \u00a0Whenever you run a new simulation in Radiance, it is not always necessary to re-write all of the initial simulation files from scratch. \u00a0These initial simulation files include both a .rad geometry file as well as a separate .pts file that contains the test point locations. \u00a0If all that you are changing in a given parametric run is the locations of the test points (like your case), it is not necessary to re-write (or reinterpret) the entire .rad geometry file. \u00a0My guess is that there is some type of check for this built into either code Mostapha wrote or radiance functions that Mostapha is calling. As such, it seems that the rad geometry file is not being re-written (or re-interpreted by radiance) completely when all that you change is the test points and this actually seems to be saving you an extra 10 seconds each time that you run the component without changing the materials or the building geometry. \u00a0Other times (like when you plug in custom radParameters), it seems that it re-writes (or re-interprets) the .rad geometry file from scratch since this file is probably affected by customized rad parameters.\n\n\n\nSo far, if this explanation is holding, it seems like there would be no concern on your end but I also recognize that the difference between these long and short simulations is giving you radiation results that are ever so slightly different from each other (by my estimates, they differ by about 0.2%). \u00a0Compared to the other types of assumptions that the radiance model is making, though, these are mere rounding errors that probably originate from the number of decimal places in the vertices of the rad geometry file. \u00a0Rather than worrying about whether your simulations are giving you the right rounding errors to give you matching results, I would encourage you to instead contemplate how much your radiance results are matching reality given all of the assumptions that you are making about the climate (with the epw file for a \"typical\" year) and with the number of light bounces in the radiance simulation. \u00a0To give you an example, I ran your model with a higher quality of simulation type (3 ambient bounces) and this gives you results that differ by 1.1% from the original simulation that you were running with only 2 ambient bounces (this is practically an order of magnitude larger than 0.2%).\n\n\n\nTo address your unease I will say that, for a long time, I also felt uneasy any time that I encountered something that seemed unpredictable in software that I was using. \u00a0Once I started coding my own stuff, though, I realized quickly that unpredictable behavior is an unavoidable aspect of all software. \u00a0There is always a tradeoff between accurate results and the time it takes to get them, which produces a multitude of possible ways to arrive at a solution. \u00a0Add into this complex situation the fact that you might have an almost infinite number of possible inputs to a given set of code.\n\n\n\nBecause of the unpredictable\u00a0multitude of cases, there is no application that is completely free from limitations and assumptions. \u00a0In this light, what ends up being more important than the actual calculation method used is the social infrastructure that is in place to help understand what is being run under the hood, hence why both Radiance and Honeybee are open source and why we try to build a robust community of support through forums like this one!\n\n-Chris\n", "attachments": [], "created_by_name": "Chris Mackey", "created_at": "August 22, 2015 at 3:46pm", "created_by": "chris", "parent_id": "reply_2985220_Comment_1346296", "id": "reply_2985220_Comment_1347417"}