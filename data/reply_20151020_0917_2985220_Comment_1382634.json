{"body": "\nSarith,\n\n\u00a0\n\n**Identify the aspect of calculations that consumes the most amount of time and resources:**\n\nBased on past projects, I expect the following computation time breakdown. This all depends on how complex the models are. If we are running multi-room E+ studies, that will take far longer to calculate.\n\n\u00a0\n\n\u00a0\n\n**Parallelizing Grasshopper**:\n\nMy first instinct is to avoid this problem by running GH on one computer only. Creating the batch files is very fast. The trick will be sending the radiance and E+ batch files to multiple computers. Perhaps a \u201cround-robin\u201d approach could send each iteration to another node on the network until all iterations are assigned. I have no idea how to do that but hope that it is something that can be executed within grasshopper, perhaps a custom code module. I think GH can set a directory for Radiance and E+ to save all final files to. We can set this to a local server location so all runs output to the same location. It will likely run slower than it would on the C:drive, but those losses are acceptable if we can get parallelization to work.\n\n\u00a0\n\nI\u2019m concerned about post-processing of the Radiance/E+ runs. For starters, Honeybee calculates DA after it runs the .ill files. This doesn\u2019t take very long, but it is a separate process that is not included in the original Radiance batch file. Any other data manipulation we intend to automatically run in GH will be left out of the batch file as well. Consolidating the results into a format that Design Explorer or Pollination can read also takes a bit of post-processing. So, it seems to me that we may want to split up the GH automation as follows:\n\nCalculate\n\nPost Processing\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nThe above workflow avoids having to parallelize GH. The consequence is that we can\u2019t parallelize any post-processing routines. This may be easier to implement in the short term, but long term we should try to parallelize everything.\n\n\u00a0\n\n**Parallelizing EnergyPlus/Radiance:**\n\nI agree that the best way to enable large numbers of iterations is to set up multiple unique runs of radiance and E+ on separate computers. I don\u2019t see the incentive to split individual runs between multiple processors because the modular nature of the iterative parametric models does this for us. Multiple unique runs will simplify the post-processing as well.\n\n\u00a0\n\nIt seems that the advantages of optimizing matrix based calculations (3-5 phase methods) are most beneficial when iterations are run in series. Is it possible for multiple iterations running on different CPUs to reference the same matrices stored in a common location? Will that enable parallel computation to also benefit from reusing pre-calculated information?\n\n\u00a0\n\n**Clustering computers and GPU based calculations:**\n\nClustering unused computers seems like a natural next step for us. Our IT guru told me that we need come kind of software to make this happen, but that he didn\u2019t know what that would be. Do you know what Penn State uses? You mentioned it is a text-only Linux based system. Can you please elaborate so I can explain to our IT department?\n\n\u00a0\n\nAccelerad is a very exciting development, especially for rpict and annual glare analysis. I\u2019m concerned that the high quality GPU\u2019s required might limit our ability to implement it on a large scale within our office. Does it still work well on standard GPU\u2019s? The computer cluster method can tap into resources we already have, which is a big advantage. Our current workflow uses image-based calcs sparingly, because grid-based simulations gather the critical information much faster. The major exception is glare. Accelerad would enable luminance-based glare metrics, especially annual glare metrics, to be more feasible within fast-paced projects. All of that is a good thing.\n\n\u00a0\n\nSo, both clusters and GPU-based calcs are great steps forward. Combining both methods would be amazing, especially if it is further optimized by the computational methods you are working on.\n\n\u00a0\n\nMoving forward, I think I need to explore if/how GH can send iterations across a cluster network of some kind and see what it will take to implement Accelerad. I assume some custom scripting will be necessary.\n", "attachments": [], "created_by_name": "Leland Curtis", "created_at": "October 20, 2015 at 09:17AM", "created_by": "LelandCurtis", "parent_id": "reply_2985220_Comment_1382308", "id": "reply_2985220_Comment_1382634"}