{"body": "\nHi Theodore,\n\nSince we are talking of building simulations, I assume that when you talk of ML you are referring to supervised learning (i.e. wherein we know what we are looking for and are not after patterns and such).\n\nI am not convinced that Machine Learning is the way to go for **deciding which (or how many)** simulations should be run. Having studied a fair bit of ML as well as Design of Experiments (DOE) in grad school, I think a hybrid approach is much more efficient.\n\nMore specifically, initial simulations based on [screening,](http://www.itl.nist.gov/div898/handbook/pri/section3/pri3346.htm) [factorial design](https://en.wikipedia.org/wiki/Factorial_experiment)\u00a0 etc. can help in identifying which simulations need to be run. This can be followed by running those simulations and then performing the \"learning\" tests (Actually you could forgo ML altogether and just tweak your simulations based on something like Response Surface Design).[](http://www.itl.nist.gov/div898/handbook/pri/section3/pri3346.htm)\n\nI learnt ML with R and DOE with Minitab, but I know for a fact that the functionality needed for such hybrid studies can now be realized with Python using [scikit](http://scikit-learn.org/stable/) and r.\n\nOf course, all of this is easier said than done, but I am hoping that someone would take up this line of investigation. Dr. [Susan Sanchez](https://scholar.google.com/citations?user=ZHnaLFUAAAAJ&amp;hl=en&amp;oi=sra) has published some very interesting research on this topic.\n\n\n\nSarith\n", "attachments": [], "created_by_name": "Sarith Subramaniam", "created_at": "November 29, 2016 at 04:33PM", "created_by": "sarith", "parent_id": "topic_1647993", "id": "reply_2985220_Comment_1648462"}